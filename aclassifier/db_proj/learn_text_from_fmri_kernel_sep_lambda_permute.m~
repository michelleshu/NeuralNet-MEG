% predict the text vectors
% Set fCrossValidate to true to test different regularization params
%
% This function uses kernel ridge regression with a linear kernel. This allows us to use the full
% brain as input features because we avoid the large inversion of the voxels/voxels matrix

function [weightMatrix, minerr, minerr_perm,pove, pove_perm] = learn_text_from_fmri_kernel_sep_lambda_permute(trainingData, trainingTargets, numPermutes)
% append a vector of ones so we can learn weights and biases together
trainingData(:,end+1) = 1;  % we shouldn't use this bias with L2 reg.
numDimensions = size(trainingData,2);
numTargetDimensions = size(trainingTargets,2);
numExamples = size(trainingData,1);

params = [.0000001 .000001 .00001 .0001 .001 .01 .1 .5 1 5 10 50 100 500 1000 10000 20000 50000 ...
    100000 500000 1000000 5000000 10000000];
%params = [.00001 .0001 .001 .01 .1  1 10 100 1000 10000];
numParams = length(params);
numTargs = size(trainingTargets,2);
numWords = size(trainingData,1);

CVerr_true = zeros(numParams, numTargs);
CVerr_perm = zeros(numPermutes,numParams,numTargs);

pove_all_true = zeros(numParams, numTargs);
pove_all_perm = zeros(numPermutes,numParams,numTargs);

permuteLabs = zeros([size(trainingTargets),numPermutes]);
for i = 1:numPermutes,
    permuteLabs(:,:,i) = trainingTargets(randperm(numExamples),:);
end

% If we do an eigendecomp first we can quickly compute the inverse for many different values
% of lambda. SVD uses X = UDV' form.
% First compute K0 = (XX' + lambda*I) where lambda = 0.
K0 = trainingData*trainingData';
[U,D,V] = svd(K0);

for i = 1:length(params)
    regularizationParam = params(i);
    
    % Now we can obtain Kinv for any lambda doing Kinv = V * (D + lambda*I)^-1 U'
    dlambda = D + regularizationParam*eye(size(D));
    dlambdaInv = diag(1 ./ diag(dlambda));
    KlambdaInv = V * dlambdaInv * U';
    
    % Compute pseudoinverse of linear kernel.
    KP = trainingData' * KlambdaInv;
    
    % Compute S matrix of Hastie Trick X*KP
    S = trainingData * KP;
    
    % Solve for weight matrix so we can compute residual
    weightMatrix = KP * trainingTargets;
    Snorm = repmat(1 - diag(S), 1, numTargetDimensions);
    preds = trainingData*weightMatrix;
    YdiffMat = (trainingTargets - preds);
    YdiffMat = YdiffMat ./ Snorm;
    CVerr_true(i,:) = (1/numExamples).*sum(YdiffMat .* YdiffMat);
    pove_all_true(i,:) = 1- sum((preds - trainingTargets).^2)./sum((repmat(mean(trainingTargets),numExamples)-trainingTargets).^2);
    %Now do it for permuted labels too.
    % could probably avoid this loop by reformatting the matrix in 2d
    weightMatrix = multiprod(KP, permuteLabs);
    perm_preds = multiprod(trainingData,weightMatrix);
    for j = 1:numPermutes,
        %         weightMatrix = KP * permuteLabs(:,:,j);
        YdiffMat = (permuteLabs(:,:,j) - perm_preds(:,:,j));
        YdiffMat = YdiffMat ./ Snorm;
        CVerr_perm(j,i,:) = (1/numExamples).*sum(YdiffMat .* YdiffMat);
        pove_all_perm(j,i,:) = 1- sum((perm_preds(:,:,j) - trainingTargets).^2)./sum((repmat(mean(trainingTargets),numExamples)-trainingTargets).^2);
    end
end

[minerr, minerrIndex] = min(CVerr_true);
pove =pove_all(minerrIndex);
for cur_targ = 1:numTargs,
    regularizationParam = params(minerrIndex(cur_targ));
    
    % got good param, now obtain weights
    dlambda = D + regularizationParam*eye(size(D));
    dlambdaInv = diag(1 ./ diag(dlambda));
    KlambdaInv = V * dlambdaInv * U';
    
    % Solve for weight matrix so we can compute residual
    weightMatrix(:,cur_targ) = trainingData' * KlambdaInv * trainingTargets(:,cur_targ);
end

minerr_all = zeros(numPermutes,numTargetDimensions);
pove_perm = zeros(numPermutes, numTargetDimensions);
for j = 1:numPermutes,
    [minerr_perm, minerrIndex_perm] =min(CVerr_perm(j,:,:),[],2);
    minerr_all(j,:) = squeeze(minerr_perm);
    pove_perm(j,:) = pove_all_perm(j,:,:),[],2));
end

return;
end

